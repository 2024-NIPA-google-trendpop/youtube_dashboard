{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved in your configured git credential helpers (osxkeychain).\n",
      "Your token has been saved to /Users/jaesolshin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "#Use 3.12.4\n",
    "import os\n",
    "import markdown\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from langchain import FAISS\n",
    "from langchain.text_splitter import SpacyTextSplitter \n",
    "from langchain.document_loaders import PyMuPDFLoader, DirectoryLoader, TextLoader\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models.base import BaseChatModel\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.docstore.document import Document\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "key_path = '/Users/jaesolshin/key/HF_TOKEN.txt'\n",
    "os.environ[\"HF_TOKEN\"] = open(key_path, 'r', encoding='utf-8').read()\n",
    "login(os.environ[\"HF_TOKEN\"], add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_system_prompt = \"\"\"\n",
    "\n",
    "소셜 미디어 중재 시스템으로서 사용자 댓글을 분류하여야 합니다.\n",
    "주제와 관련된 의견을 분석하여 다음 범주 중 하나로 분류합니다:\n",
    "\n",
    "Positive(기쁨, 황홀, 신뢰, 지지, 공감, 기대 등 긍정적 감정이나 태도 )\n",
    "Negetive(분노, 공포, 슬픔, 혐오, 짜증, 지루함 등 부정적 감정이나 태도)\n",
    "\n",
    "이 의견이 위의 범주 중 어느 것에도 부합하지 않는 경우, 다음과 같이 분류합니다:\n",
    "Neutral(중립)\n",
    "\n",
    "설명 없이 카테고리만 숫자형식의 응답으로 제공합니다.\n",
    "\n",
    "'2' (Positive), '1' (Neutral), '0' (Negetive)\n",
    "\"\"\"\n",
    "\n",
    "classification_template = \"\"\"\n",
    "아티스트: 뉴진스\n",
    "댓글: 지금까지 위버스에 돈 다 쳐바른 내가 한심하다 팬들 돈 뽑아먹으니까 좋니~?\t\n",
    "클래스: 0\n",
    "\n",
    "아티스트: 뉴진스\n",
    "댓글: 혜인아 너 왜케 예뻐 .. ? 수퍼내추럴에서 또한번 어텐션만큼 찢었어 ㅠ ㅠ 사랑해...\t\n",
    "클래스: 2\n",
    "\n",
    "아티스트: aespa\n",
    "댓글: 나비스 나올 때 엑소 왓이즈러브 생각나네\n",
    "클래스: 1\n",
    "\n",
    "Topic: {group}\n",
    "Comment: {comment}\n",
    "Class:\n",
    "\"\"\"\n",
    "\n",
    "# 감정분류를 위한 주제와 댓글을 정의\n",
    "sample_group= \"aespa\"\n",
    "sample_comment = \"aespa Forever커버곡좋다꽃길만걷고올해음악방송1위가자\"\n",
    "\n",
    "# 주제와 댓글을 바탕으로 분류 프롬프트 생성\n",
    "prompt = classification_template.format(group=sample_group, comment=sample_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239a69ec06d84b96b15a4810697e514a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "\n",
    "model_id = \"beomi/gemma-ko-2b\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/gemma-ko-2b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"mps\",\n",
    "    torch_dtype=dtype,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nComment: 엑소 왓이즈러브 뮤비보고 울었다 ㅠㅠ 엑소 왓이즈러브 뮤비보고 울었다 ㅠㅠ 엑소 왓이즈러브 뮤비보고 울었다 ㅠㅠ 엑소 왓이즈러브 뮤비보고 울었다 ㅠㅠ 엑소 왓이즈러브 뮤비보고 울었다 ㅠㅠ 엑소 왓이즈러브 뮤비보고 울었다 ㅠㅠ 엑소 왓이즈러브 뮤비보고 울었다 ㅠㅠ 엑소 왓이즈러'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0]).split('Class:')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":\n",
      "Comment: 엑소 왓이즈러브 뮤비보고 울었다 ㅠㅠ 엑소 왓이즈러브 뮤비보고 울었다 ㅠㅠ 엑소 왓이즈러브 뮤비보고 울었다 ㅠㅠ 엑소 왓이즈러브 뮤비보고 울었다 ㅠㅠ 엑소 왓이즈러브 뮤비보고 울었다 ㅠㅠ 엑소 왓이즈러브 뮤비보고 울었다 ㅠㅠ 엑소 왓이즈러브 뮤비보고 울었다 ㅠㅠ 엑소 왓이즈러\n"
     ]
    }
   ],
   "source": [
    "answer = tokenizer.decode(outputs[0]).split('Class')[1]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/generation/utils.py:1710\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1706\u001b[0m \u001b[38;5;66;03m# 3. Define model inputs\u001b[39;00m\n\u001b[1;32m   1707\u001b[0m inputs_tensor, model_input_name, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_model_inputs(\n\u001b[1;32m   1708\u001b[0m     inputs, generation_config\u001b[38;5;241m.\u001b[39mbos_token_id, model_kwargs\n\u001b[1;32m   1709\u001b[0m )\n\u001b[0;32m-> 1710\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[43minputs_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1712\u001b[0m device \u001b[38;5;241m=\u001b[39m inputs_tensor\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m   1713\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_special_tokens(generation_config, kwargs_has_attention_mask, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "model.generate(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "# 시스템 프롬프트와 classification_template를 하나의 프롬프트로 결합\n",
    "classification_system_prompt = \"\"\"\n",
    "소셜 미디어 중재 시스템으로서 사용자 댓글을 분류하여야 합니다.\n",
    "주제와 관련된 의견을 분석하여 다음 범주 중 하나로 분류합니다:\n",
    "\n",
    "Positive(기쁨, 황홀, 신뢰, 지지, 공감, 기대 등 긍정적 감정이나 태도 )\n",
    "Negative(분노, 공포, 슬픔, 혐오, 짜증, 지루함 등 부정적 감정이나 태도)\n",
    "\n",
    "이 의견이 위의 범주 중 어느 것에도 부합하지 않는 경우, 다음과 같이 분류합니다:\n",
    "Neutral(중립)\n",
    "\n",
    "설명 없이 카테고리만 숫자 형식으로 응답합니다:\n",
    "\n",
    "'2' (Positive), '1' (Neutral), '0' (Negative)\n",
    "\"\"\"\n",
    "\n",
    "classification_template = \"\"\"\n",
    "아티스트: 뉴진스\n",
    "댓글: 지금까지 위버스에 돈 다 쳐바른 내가 한심하다 팬들 돈 뽑아먹으니까 좋니~?\t\n",
    "클래스: 0\n",
    "\n",
    "아티스트: 뉴진스\n",
    "댓글: 혜인아 너 왜케 예뻐 .. ? 수퍼내추럴에서 또한번 어텐션만큼 찢었어 ㅠ ㅠ 사랑해...\t\n",
    "클래스: 2\n",
    "\n",
    "아티스트: aespa\n",
    "댓글: 나비스 나올 때 엑소 왓이즈러브 생각나네\n",
    "클래스: 1\n",
    "\n",
    "아티스트: {group}\n",
    "댓글: {comment}\n",
    "클래스:\n",
    "\"\"\"\n",
    "\n",
    "# 감정 분류를 위한 주제와 댓글을 정의\n",
    "sample_group = \"aespa\"\n",
    "sample_comment = \"사랑해\"\n",
    "\n",
    "# 프롬프트 생성\n",
    "prompt = classification_system_prompt + classification_template.format(group=sample_group, comment=sample_comment)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"beomi/gemma-ko-2b\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "\n",
    "# 입력 텍스트를 토큰화하고 모델에 입력\n",
    "inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(input_ids=inputs, max_new_tokens=150)\n",
    "\n",
    "# 결과 텍스트 디코딩\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 클래스 값 추출\n",
    "if '클래스:' in generated_text:\n",
    "    answer = generated_text.split('클래스:')[1].strip().split()[0]  # '클래스:' 이후 첫 번째 값 추출\n",
    "else:\n",
    "    answer = \"No class found\"\n",
    "\n",
    "print(f\"Predicted class: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "소셜 미디어 중재 시스템으로서 사용자 댓글을 분류하여야 합니다.\n",
      "주제와 관련된 의견을 분석하여 다음 범주 중 하나로 분류합니다:\n",
      "\n",
      "Positive(기쁨, 황홀, 신뢰, 지지, 공감, 기대 등 긍정적 감정이나 태도 )\n",
      "Negative(분노, 공포, 슬픔, 혐오, 짜증, 지루함 등 부정적 감정이나 태도)\n",
      "\n",
      "이 의견이 위의 범주 중 어느 것에도 부합하지 않는 경우, 다음과 같이 분류합니다:\n",
      "Neutral(중립)\n",
      "\n",
      "설명 없이 카테고리만 숫자 형식으로 응답합니다:\n",
      "\n",
      "'2' (Positive), '1' (Neutral), '0' (Negative)\n",
      "\n",
      "다음은 예시입니다:\n",
      "\n",
      "\n",
      "\n",
      "아티스트: 뉴진스\n",
      "댓글: 지금까지 위버스에 돈 다 쳐바른 내가 한심하다 팬들 돈 뽑아먹으니까 좋니~?\n",
      "클래스: 0\n",
      "\n",
      "\n",
      "\n",
      "아티스트: 뉴진스\n",
      "댓글: 혜인아 너 왜케 예뻐 .. ? 수퍼내추럴에서 또한번 어텐션만큼 찢었어 ㅠ ㅠ 사랑해...\n",
      "클래스: 2\n",
      "\n",
      "\n",
      "\n",
      "아티스트: aespa\n",
      "댓글: 나비스 나올 때 엑소 왓이즈러브 생각나네\n",
      "클래스: 1\n",
      "\n",
      "\n",
      "\n",
      "아티스트: aespa\n",
      "댓글: aespa Forever커버곡좋다꽃길만걷고올해음악방송1위가자\n",
      "클래스:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# 예시 목록 정의 (Few-shot 샘플)\n",
    "examples = [\n",
    "    {\"group\": \"뉴진스\", \"comment\": \"지금까지 위버스에 돈 다 쳐바른 내가 한심하다 팬들 돈 뽑아먹으니까 좋니~?\", \"class\": \"0\"},\n",
    "    {\"group\": \"뉴진스\", \"comment\": \"혜인아 너 왜케 예뻐 .. ? 수퍼내추럴에서 또한번 어텐션만큼 찢었어 ㅠ ㅠ 사랑해...\", \"class\": \"2\"},\n",
    "    {\"group\": \"aespa\", \"comment\": \"나비스 나올 때 엑소 왓이즈러브 생각나네\", \"class\": \"1\"},\n",
    "]\n",
    "\n",
    "# 기본 예시를 활용한 few-shot 프롬프트 템플릿 정의\n",
    "example_template = \"\"\"\n",
    "아티스트: {group}\n",
    "댓글: {comment}\n",
    "클래스: {class}\n",
    "\"\"\"\n",
    "\n",
    "# PromptTemplate을 사용해 각 예시를 구성하는 템플릿을 정의\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"group\", \"comment\", \"class\"],\n",
    "    template=example_template,\n",
    ")\n",
    "\n",
    "# 주어진 예시들과 함께 사용될 FewShotPromptTemplate 정의\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    examples=examples,  # 위에서 정의한 예시 리스트\n",
    "    example_prompt=example_prompt,  # 각 예시에 대한 프롬프트 템플릿\n",
    "    prefix=\"\"\"\n",
    "소셜 미디어 중재 시스템으로서 사용자 댓글을 분류하여야 합니다.\n",
    "주제와 관련된 의견을 분석하여 다음 범주 중 하나로 분류합니다:\n",
    "\n",
    "Positive(기쁨, 황홀, 신뢰, 지지, 공감, 기대 등 긍정적 감정이나 태도 )\n",
    "Negative(분노, 공포, 슬픔, 혐오, 짜증, 지루함 등 부정적 감정이나 태도)\n",
    "\n",
    "이 의견이 위의 범주 중 어느 것에도 부합하지 않는 경우, 다음과 같이 분류합니다:\n",
    "Neutral(중립)\n",
    "\n",
    "설명 없이 카테고리만 숫자 형식으로 응답합니다:\n",
    "\n",
    "'2' (Positive), '1' (Neutral), '0' (Negative)\n",
    "\n",
    "다음은 예시입니다:\n",
    "\"\"\",  # 프롬프트 앞부분에 들어갈 시스템 지시사항\n",
    "    suffix=\"\"\"\n",
    "아티스트: {group}\n",
    "댓글: {comment}\n",
    "클래스:\n",
    "\"\"\",  # 실제 주어진 데이터에 대해 모델이 분류해야 하는 질문\n",
    "    input_variables=[\"group\", \"comment\"],  # 프롬프트에서 사용할 변수\n",
    ")\n",
    "\n",
    "# 실제 사용할 데이터\n",
    "sample_group = \"aespa\"\n",
    "sample_comment = \"aespa Forever커버곡좋다꽃길만걷고올해음악방송1위가자\"\n",
    "\n",
    "# 최종 프롬프트 생성\n",
    "final_prompt = few_shot_prompt.format(group=sample_group, comment=sample_comment)\n",
    "\n",
    "# 생성된 프롬프트 출력\n",
    "print(final_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "# 입력 텍스트를 토큰화하고 모델에 입력\n",
    "inputs = tokenizer.encode(final_prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(input_ids=inputs, max_new_tokens=150)\n",
    "\n",
    "# 결과 텍스트 디코딩\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 클래스 값 추출\n",
    "if '클래스:' in generated_text:\n",
    "    answer = generated_text.split('클래스:')[1].strip().split()[0]  # '클래스:' 이후 첫 번째 값 추출\n",
    "else:\n",
    "    answer = \"No class found\"\n",
    "\n",
    "print(f\"Predicted class: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
